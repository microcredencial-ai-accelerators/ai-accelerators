# Microcredencial AI Accelerators for Reconfigurable Technologies: Introduction to Neural Network Acceleration on FPGAs
## [Back to index](index.md)

## Introduction
Field-Programmable Gate Arrays (FPGAs) play an increasingly important role in accelerating AI workloads, offering a powerful combination of parallelism, low latency, and energy efficiency. Unlike fixed-function hardware accelerators, FPGAs can be reconfigured to match the computational structure of a neural network, making them ideal for custom, application-specific AI deployments.

This module introduces the fundamentals of implementing and accelerating neural networks on FPGAs using OpenCL and the Intel FPGA SDK for OpenCL. Through hands-on examples with the DE10-Nano (Cyclone V SoC) platform, you will learn how to translate neural network operations into FPGA-friendly compute kernels.

By the end of this module, you will understand how to:

- Work with OpenCL in the context of FPGA acceleration
- Map neural network computations to FPGA kernels
- Implement fully connected (FC) and convolutional neural network (CNN) layers in OpenCL
- Build, compile, and execute OpenCL kernels using the Intel FPGA SDK for OpenCL

This module provides the foundation for designing custom FPGA-based neural network accelerators, bridging the gap between high-level AI models and low-level hardware execution.

## [Module 3.1: Introduction to OpenCL and Intel FPGA SDK for OpenCL Using DE10-Nano (Cyclone V SoC)](module3-accelerators-opencl.md)
